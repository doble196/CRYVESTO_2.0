{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83720b92-6c6b-43e2-b37d-3bc6200d8866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snorkel in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (0.9.8)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.2.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (1.11.0)\n",
      "Requirement already satisfied: networkx<2.7,>=2.2 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (2.6.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.33.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (4.62.3)\n",
      "Requirement already satisfied: tensorboard<2.7.0,>=2.0.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (2.6.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (1.7.3)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.0.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (1.3.4)\n",
      "Requirement already satisfied: scikit-learn<0.25.0,>=0.20.2 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (0.24.2)\n",
      "Requirement already satisfied: numpy<1.20.0,>=1.16.5 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (1.19.5)\n",
      "Requirement already satisfied: munkres>=1.0.6 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from snorkel) (1.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from pandas<2.0.0,>=1.0.0->snorkel) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from pandas<2.0.0,>=1.0.0->snorkel) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas<2.0.0,>=1.0.0->snorkel) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from scikit-learn<0.25.0,>=0.20.2->snorkel) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from scikit-learn<0.25.0,>=0.20.2->snorkel) (1.1.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (1.35.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (2.0.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (1.46.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (2.27.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (58.0.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (3.19.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from tensorboard<2.7.0,>=2.0.0->snorkel) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7.0,>=2.0.0->snorkel) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7.0,>=2.0.0->snorkel) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7.0,>=2.0.0->snorkel) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7.0,>=2.0.0->snorkel) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7.0,>=2.0.0->snorkel) (4.8.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7.0,>=2.0.0->snorkel) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7.0,>=2.0.0->snorkel) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7.0,>=2.0.0->snorkel) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7.0,>=2.0.0->snorkel) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7.0,>=2.0.0->snorkel) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7.0,>=2.0.0->snorkel) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from torch<2.0.0,>=1.2.0->snorkel) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.7.0,>=2.0.0->snorkel) (3.7.0)\n",
      "Requirement already satisfied: textblob in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: joblib in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from nltk>=3.1->textblob) (2021.11.2)\n",
      "Requirement already satisfied: importlib-metadata in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from click->nltk>=3.1->textblob) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from importlib-metadata->click->nltk>=3.1->textblob) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/ashokpandey/opt/anaconda3/envs/dev/lib/python3.7/site-packages (from importlib-metadata->click->nltk>=3.1->textblob) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "#install needed packages\n",
    "!pip install snorkel\n",
    "!pip install textblob\n",
    "#import libraries and modules\n",
    "#from google.colab import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226f017-dbe2-4517-a57d-d004ba0d125f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee94e77-750f-4923-ac86-be3763662077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a399b33e-8b76-487e-b8ea-6e94ca13c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snorkel\n",
    "from snorkel.labeling import LabelingFunction\n",
    "import re\n",
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import LFAnalysis\n",
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "from snorkel.labeling import labeling_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4f7968-1765-4529-adf9-01f1cb8a049b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wg/dbmj0v8x4k91_3hgslxt96p40000gn/T/ipykernel_53749/1880683020.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#NLP packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#NLP packages\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "punc = string.punctuation\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13afb2c-f237-4254-bf2b-346a016ff1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supervised learning\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "##Deep learning libraries and APIs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3331caee-9c14-4e58-8a84-f112ab354a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19227 entries, 0 to 19226\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Date      19227 non-null  object\n",
      " 1   text      19227 non-null  object\n",
      " 2   Category  19227 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 450.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#uplaod the data from your local directory\n",
    "#uploaded = files.upload()\n",
    "# store the dataset as a Pandas Dataframe\n",
    "#df = pd.read_csv(io.BytesIO(uploaded['wsj_headlines.csv']))\n",
    "df = pd.read_csv('wsj_headlines.csv')\n",
    "#conduct some data cleaning\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df = df.rename(columns = {'Headline': 'text'})\n",
    "df['text'] = df['text'].astype(str)\n",
    "#check the data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "931061ed-023f-42f1-8f31-15a0f86e1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants to represent the class labels :positive, negative, and abstain\n",
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1\n",
    "#define function which looks into the input words to represent a proper label\n",
    "def keyword_lookup(x, keywords, label):  \n",
    "    if any(word in x.text.lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "#define function which assigns a correct label\n",
    "def make_keyword_lf(keywords, label=POSITIVE):\n",
    "    return LabelingFunction(\n",
    "        name=f\"keyword_{keywords[0]}\",\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(keywords=keywords, label=label))\n",
    "#resource: https://www.snorkel.org/use-cases/01-spam-tutorial#3-writing-more-labeling-functions\n",
    "#these two lists can be further extended \n",
    "\"\"\"positive news might contain the following words' \"\"\"\n",
    "keyword_positive = make_keyword_lf(keywords=['boosts', 'great', 'develops', 'promising', 'ambitious', 'delighted', 'record', 'win', 'breakthrough', 'recover', 'achievement', 'peace', 'party', 'hope', 'flourish', 'respect', 'partnership', 'champion', 'positive', 'happy', 'bright', 'confident', 'encouraged', 'perfect', 'complete', 'assured' ])\n",
    "\"\"\"negative news might contain the following words\"\"\"\n",
    "keyword_negative = make_keyword_lf(keywords=['war','solidiers', 'turmoil', 'injur','trouble', 'aggressive', 'killed', 'coup', 'evasion', 'strike', 'troops', 'dismisses', 'attacks', 'defeat', 'damage', 'dishonest', 'dead', 'fear', 'foul', 'fails', 'hostile', 'cuts', 'accusations', 'victims',  'death', 'unrest', 'fraud', 'dispute', 'destruction', 'battle', 'unhappy', 'bad', 'alarming', 'angry', 'anxious', 'dirty', 'pain', 'poison', 'unfair', 'unhealthy'\n",
    "                                              ], label=NEGATIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50531990-af01-4fb0-9210-710bdeb9f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up a preprocessor function to determine polarity & subjectivity using textlob pretrained classifier \n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x\n",
    "#find polarity\n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return POSITIVE if x.polarity > 0.6 else ABSTAIN\n",
    "#find subjectivity \n",
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return POSITIVE if x.subjectivity >= 0.5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfbff435-2157-4d31-9065-3035c8005f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 19227/19227 [00:11<00:00, 1687.18it/s]\n",
      "INFO:root:Computing O...\n",
      "INFO:root:Estimating \\mu...\n",
      "  0%|                                                | 0/100 [00:00<?, ?epoch/s]INFO:root:[0 epochs]: TRAIN:[loss=0.011]\n",
      "  1%|▍                                       | 1/100 [00:00<00:10,  9.89epoch/s]INFO:root:[10 epochs]: TRAIN:[loss=0.004]\n",
      "INFO:root:[20 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[30 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[40 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[50 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[60 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[70 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[80 epochs]: TRAIN:[loss=0.000]\n",
      "INFO:root:[90 epochs]: TRAIN:[loss=0.000]\n",
      "100%|█████████████████████████████████████| 100/100 [00:00<00:00, 684.12epoch/s]\n",
      "INFO:root:Finished Training\n"
     ]
    }
   ],
   "source": [
    "#combine all the labeling functions \n",
    "lfs = [keyword_positive, keyword_negative, textblob_polarity, textblob_subjectivity ]\n",
    "#apply the lfs on the dataframe\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_snorkel = applier.apply(df=df)\n",
    "#apply the label model\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "#fit on the data\n",
    "label_model.fit(L_snorkel)\n",
    "#predict and create the labels\n",
    "df[\"label\"] = label_model.predict(L=L_snorkel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01bac34b-0391-4948-8fad-f2548c6af9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3098\n",
       "0    1209\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtering out unlabeled data points\n",
    "df= df.loc[df.label.isin([0,1]), :]\n",
    "#find the label counts \n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529686cc-f6e6-4ff2-bcad-4c59417dc632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy of the dataframe\n",
    "data = df.copy()\n",
    "#define a function which handles the text preprocessing \n",
    "def preparation_text_data(data):\n",
    "    \"\"\"\n",
    "    This pipeline prepares the text data, conducting the following steps:\n",
    "    1) Tokenization\n",
    "    2) Lemmatization\n",
    "    4) Removal of stopwords\n",
    "    5) Removal of punctuation\n",
    "    \"\"\"\n",
    "    # initialize spacy object\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # select raw text\n",
    "    raw_text = data.text.values.tolist()\n",
    "    # tokenize\n",
    "    tokenized_text = [[nlp(i.lower().strip())] for i in tqdm(raw_text)]\n",
    "    #define the punctuations and stop words\n",
    "    punc = string.punctuation \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #lemmatize, remove stopwords and punctuationd\n",
    "    corpus = []\n",
    "    for doc in tqdm(tokenized_text):\n",
    "        corpus.append([word.lemma_ for word in doc[0] if (word.lemma_ not in stop_words and word.lemma_ not in punc)])\n",
    "    # add prepared data to df\n",
    "    data[\"text\"] = corpus\n",
    "    return data\n",
    "#apply the data preprocessing function\n",
    "data =  preparation_text_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375cc3cc-e154-4096-a6b4-7e594dcb17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_representation(data):\n",
    "  tfidf_vect = TfidfVectorizer()\n",
    "  data['text'] = data['text'].apply(lambda text: \" \".join(set(text)))\n",
    "  X_tfidf = tfidf_vect.fit_transform(data['text'])\n",
    "  print(X_tfidf.shape)\n",
    "  print(tfidf_vect.get_feature_names())\n",
    "  X_tfidf = pd.DataFrame(X_tfidf.toarray())\n",
    "  return X_tfidf\n",
    "#apply the TFIDV function\n",
    "X_tfidf = text_representation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffe631-8065-4d36-b708-746686641922",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= X_tfidf\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "#fit Log Regression Model\n",
    "clf= LogisticRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172b096-369b-45eb-894b-be8e4a6b056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [\"The US imposes sanctions on Rassia because of the Ukranian war\"]\n",
    "tf = TfidfVectorizer()\n",
    "tfdf = tf.fit_transform(data['text'])\n",
    "vect = pd.DataFrame(tf.transform(new_data).toarray())\n",
    "new_data = pd.DataFrame(vect)\n",
    "logistic_prediction = clf.predict(new_data)\n",
    "print(logistic_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c8a62-0b57-4575-81dd-86dbc87b041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##store headlines and labels in respective lists\n",
    "text = list(data['text'])\n",
    "labels = list(data['label'])\n",
    "##sentences\n",
    "training_text = text[0:15000]\n",
    "testing_text = text[15000:]\n",
    "##labels\n",
    "training_labels = labels[0:15000]\n",
    "testing_labels = labels[15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1872c3-cded-401f-a49b-1e08a7051f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess \n",
    "tokenizer = Tokenizer(num_words=10000, oov_token= \"<OOV>\")\n",
    "tokenizer.fit_on_texts(training_text)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequences = tokenizer.texts_to_sequences(training_text)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=120, padding='post', truncating='post')\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_text)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=120, padding='post', truncating='post')\n",
    "# convert lists into numpy arrays to make it work with TensorFlow \n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97d5da7-b908-4637-a649-04e95b460883",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16, input_length=120),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "##compile the model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997489a9-0db7-4a3d-b9f6-1f6633a6cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(training_padded, \n",
    "                    training_labels, \n",
    "                    epochs=num_epochs, \n",
    "                    validation_data=(testing_padded, testing_labels), \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7d3d9-d848-45aa-8bb0-497c79552c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_headline = [\"The US imposes sanctions on Rassia because of the Ukranian war\"]\n",
    "##prepare the sequences of the sentences in question\n",
    "sequences = tokenizer.texts_to_sequences(new_headline)\n",
    "padded_seqs = pad_sequences(sequences, maxlen=120, padding='post', truncating='post')\n",
    "print(model.predict(padded_seqs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
